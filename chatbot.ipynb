{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc4dc61",
   "metadata": {},
   "source": [
    "Import  Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f1f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae58f3fa",
   "metadata": {},
   "source": [
    "Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d9d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Load API keys\n",
    "os.environ['HF_TOKEN'] = os.getenv(\"HF_TOKEN\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daba467",
   "metadata": {},
   "source": [
    "Initialize Embeddings and LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9030bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\Downloads\\PDF_Question_Answer_Chatbot\\pdfenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Intialize Embeddings model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#  Intialize LLM model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6e9f5",
   "metadata": {},
   "source": [
    "Session History Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c4b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"default_session\"\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session: str) -> BaseChatMessageHistory:\n",
    "    if session not in store:\n",
    "        store[session] = ChatMessageHistory()\n",
    "    return store[session]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1522aad8",
   "metadata": {},
   "source": [
    "Load PDF Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab41425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(pdf_paths):\n",
    "    documents = []\n",
    "    successful_loads = 0\n",
    "    for path in pdf_paths:\n",
    "        path = path.strip()\n",
    "        if os.path.exists(path) and path.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(path)\n",
    "            docs = loader.load()\n",
    "            documents.extend(docs)\n",
    "            successful_loads += 1\n",
    "            print(f\"Successfully loaded {path} with {len(docs)} pages\")\n",
    "        else:\n",
    "            print(f\"Error: File not found or not a PDF: {path}\")\n",
    "    print(f\"\\nSuccessfully loaded {successful_loads} out of {len(pdf_paths)} PDFs\")\n",
    "    print(f\"Total pages loaded: {len(documents)}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5915d9",
   "metadata": {},
   "source": [
    "Input Documents/PDF file Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827499b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded C:\\Users\\Administrator\\Downloads\\PDF_Question_Answer_Chatbot\\NIPS-2017-attention-is-all-you-need-Paper.pdf with 11 pages\n",
      "\n",
      "Successfully loaded 1 out of 1 PDFs\n",
      "Total pages loaded: 11\n"
     ]
    }
   ],
   "source": [
    "pdf_path_input = input(\"Enter PDF file paths: \")\n",
    "pdf_path_list = [path.strip() for path in pdf_path_input.split(',') if path.strip()]\n",
    "documents = load_documents(pdf_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea64dfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 78 document chunks\n"
     ]
    }
   ],
   "source": [
    "# Split Documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=75,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(splits)} document chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0997337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorstore to store vector embeddings using FAISS\n",
    "vectorstore = FAISS.from_documents(splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d31d08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e9a5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt for Contextualized Query\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Given a chat history and the latest user question which might reference context...\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de326772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History-Aware Retriever\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm,\n",
    "    retriever,\n",
    "    contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6a6c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question-Annswer Prompt\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful assistant answering questions about documents. \"\n",
    "        \"Use the following retrieved context to answer the user's question. \"\n",
    "        \"\\n\\n{context}\\n\\n\"\n",
    "        \"Instructions:\\n1. If context is enough, answer.\\n2. If not, say so...\\n3. Ask for clarification if unclear.\\n\"\n",
    "    )),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a589abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question-Answer Chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70559423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Retriever and QA Chain into a RAG Chain\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f5a5df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine/wrap up rag QA chain with Message History Handling\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37426f1b",
   "metadata": {},
   "source": [
    "Ask Questions to get Responses based on external knowledge source (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "944b1b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating response...\n",
      "\n",
      "Assistant: Based on the provided context, the Transformer refers to a specific neural network architecture, particularly in the field of natural language processing. It's a type of sequence-to-sequence model that's primarily used for machine translation tasks, such as English-to-German translation.\n",
      "\n",
      "The Transformer architecture, as shown in Figure 1, consists of an encoder and a decoder. It uses self-attention mechanisms to relate signals from different input or output positions, allowing it to model complex dependencies between distant positions in a sequence.\n",
      "\n",
      "The key innovations of the Transformer architecture include:\n",
      "\n",
      "1. **Multi-Head Attention**: This allows the model to attend to different aspects of the input sequence simultaneously, improving its ability to capture complex relationships.\n",
      "\n",
      "2. **Self-Attention**: This mechanism enables the model to relate signals from different positions in the input sequence, reducing the computational complexity of modeling dependencies between distant positions.\n",
      "\n",
      "3. **Encoder-Decoder Structure**: The Transformer uses a stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, as shown in Figure 1.\n",
      "\n",
      "Overall, the Transformer architecture has been shown to be effective in machine translation tasks and has paved the way for further research in natural language processing.\n",
      "Generating response...\n",
      "\n",
      "Assistant: According to the provided context, self-attention is an attention mechanism that relates different positions of a single sequence in order to compute a representation of the sequence. \n",
      "\n",
      "In other words, self-attention allows the model to attend to all positions in the input sequence simultaneously and weigh their importance. This is different from traditional recurrent neural network (RNN) architectures, which only consider the previous elements in the sequence when making predictions.\n",
      "\n",
      "In the context of the Transformer architecture, self-attention is used in both the encoder and decoder. In the encoder, self-attention allows each position in the input sequence to attend to all other positions in the sequence. Similarly, in the decoder, self-attention allows each position in the output sequence to attend to all other positions in the output sequence.\n",
      "\n",
      "Self-attention has been shown to be effective in a variety of natural language processing tasks, including reading comprehension, abstractive summarization, textual entailment, and learning task-independent sentence representations.\n",
      "Generating response...\n",
      "\n",
      "Assistant: According to the provided context, an optimizer is an algorithm used to update the model's parameters during the training process. In this specific case, the optimizer used is the Adam optimizer.\n",
      "\n",
      "The Adam optimizer is a popular stochastic gradient descent algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient. It is an extension of the stochastic gradient descent algorithm that adapts the learning rate for each parameter based on the magnitude of the gradient.\n",
      "\n",
      "The Adam optimizer has two main advantages:\n",
      "\n",
      "1. **Adaptive learning rate**: Adam adapts the learning rate for each parameter based on the magnitude of the gradient, which helps in faster convergence.\n",
      "2. **Moment estimation**: Adam uses the moving average of the gradient (first moment) and the squared gradient (second moment) to update the parameters, which helps in stabilizing the learning process.\n",
      "\n",
      "The Adam optimizer is widely used in deep learning models, including those for natural language processing, computer vision, and speech recognition.\n",
      "\n",
      "In the context of the provided text, the Adam optimizer is used with specific hyperparameters: β1 = 0.9, β2 = 0.98, and ϵ = 10^(-9). These hyperparameters control the learning rate, the decay rates for the first and second moments, and the epsilon value for numerical stability, respectively.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"\\nYour question (type 'exit' to quit): \").strip()\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    if not user_input:\n",
    "        print(\"Please enter a question.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        print(\"Generating response...\")\n",
    "        response = conversational_rag_chain.invoke(\n",
    "            {\"input\": user_input},\n",
    "            config={\"configurable\": {\"session_id\": session_id}},\n",
    "        )\n",
    "        print(\"\\nAssistant:\", response[\"answer\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b31ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
